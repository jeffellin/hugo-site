+++
title = "Integrating Jenkins, Tanzu Build Service and ArgoCD"
date = 2021-05-16T13:35:15-04:00
tags = ["kubernetes","Tanzu"]
featured_image = ""
description = "Integrating Jenkins, Tanzu Build Service and ArgoCD"
draft = "false"
+++


This is part two in a series of post discussing how to integrate integrate Jenkins, Tanzu Build Service, and ArgoCD.

* [Part 1: Iterative Development](https://www.ellin.com/2021/05/16/integrating-jenkins-tanzu-build-service-and-argocd/)
* Part 2: Intake of Buildpack updates
* Part 3: Promoting code to production

## Intake of Buildpack updates

An important feature of Tanzu Build Service is the ability to rebuild containers when operating system or  runtimes such as Java become available.

This is a fairly simple process when your container fleet only conists of one or two images.  However as the use of containers grows there may be dozens if not hundres of containers that must be rebuilt on a fairly frequent basis. 

The rate of that fixes become available will result in never ending toil for teams to rebuild containers.  If this process is not automated it risks consuming the teams time and thus they are not free to develop new features to add business value.


# Importing a new Buildpack or Stack

![Window_and_localhost_8080_hello-world](/wp-content/uploads/2021/TBS-Dev-Integration.png)

The above diagram depicts the flow when a developer commits new code. 

1. An update to the Tanzu Build Service artifacts,  including Buildpacks, ClusterBuilders and ClusterStacks is uploaded to the build server.
2. Each image that The Build Service is tracking will automatically be rebuilt and push to an image repository.
3. Since we are using Harbor the image can now be scanned for vulnerabilities using Trivy or Clair.  If the image passes muster a signal is sent to Jenkins via a webhook to tell it to update the kustomize manifest. While these changes could go directly to production, most organizations will want to stage them in a pre prod environment or use a progressive or canary rollout to test them.
4. ArgoCD applies the latest configuration to the desired clusters. If needed the change can be rolled back using ArgoCD or reverting a commit to the GitOps repo.

*_Argo Out of Sync_*

![Argo](/wp-content/uploads/2021/Applications-Argo.png)

Any differences between the GIT Baseline and what is deployed by Argo are reconciled automatically.  Logging in Argo will show when and why artifacts changed.

*_Argo visits service changed_*

![Argo](/wp-content/uploads/2021/Applications-Argooos.png)


## Implementing This.

Although any continuous delivery tool could be used to implement this workflow, I chose Jenkins for its ubiquity. If you want to follow along and run my pipelines, you will want to make sure you have Jenkins configured to run builds inside Kubernetes containers.

### Jenkins Prerequisites

For this flow to work we will need a few extra Jenkins plugins in addition to the ones listed in the previous post.
**TODO Need correct names
 - generic-web-hook:1.29.2
 - script utils
 
As before I have included the versions of the plugins I am using. The Jenkins plugin ecosystem is very volatile, so version numbers may vary for you over time.

### App Seed

In the Spirit of Configuration as Code, I have a groovy script that is used to seed jobs within Jenkins. Adding a new Job is as simple as adding a new entry to the script and committing the changes to GIT. In the code below, I have the name of the project and the name of the pipeline file that should be used to build the result. Since all my apps are Spring Boot, they all can use the same pipeline. A pipeline file is nothing more than a Groovy script that instructs Jenkins on executing a job.

```
def apps = [
 'spring-petclinic-vets-service': [
 buildPipeline: 'ci/jenkins/pipelines/spring-boot-app.pipeline'
 ],
 'spring-petclinic-api-gateway': [
 buildPipeline: 'ci/jenkins/pipelines/spring-boot-app.pipeline'
 ],
 'spring-petclinic-visits-service': [
 buildPipeline: 'ci/jenkins/pipelines/spring-boot-app.pipeline'
 ],
 'spring-petclinic-httpbin': [
 buildPipeline: 'ci/jenkins/pipelines/spring-boot-app.pipeline'
 ],
 'spring-petclinic-config-server': [
 buildPipeline: 'ci/jenkins/pipelines/spring-boot-app.pipeline'
 ]
]


apps.each { name, appInfo ->


 pipelineJob(name) {
 description("Job to build '$name'. Generated by the Seed Job, please do not change !!!")
 environmentVariables(
 APP_NAME: name
 )
 definition {
 cps {
 script(readFileFromWorkspace(appInfo.buildPipeline))
 sandbox()
 }
 } 
 triggers {
 scm('* * * * *') 
 }
 properties{
 disableConcurrentBuilds()
 }
 }
}
```
The complete script is available [here](https://raw.githubusercontent.com/jeffellin/spring-petclinic-microservices/k8s/ci/jenkins/apps.groovy)

### The Pipeline

The pipeline implements several stages. 

1. Fetch from GitHub
2. Create Image
3. Update Deployment Manifest

All of these steps are performed in a clean docker container as defined in the pod template.

```
apiVersion: v1
kind: Pod
metadata:
 labels:
 app.kubernetes.io/name: jenkins-build
 app.kubernetes.io/component: jenkins-build
 app.kubernetes.io/version: "1"
spec:
 volumes:
 - name: secret-volume
 secret:
 secretName: pks-cicd 
 hostAliases:
 - ip: 192.168.1.154
 hostnames:
 - "small.pks.ellin.net"
 - ip: 192.168.1.80
 hostnames:
 - "harbor.ellin.net"
 containers:
 - name: k8s
 image: harbor.ellin.net/library/docker-build
 command:
 - sleep
 env:
 - name: KUBECONFIG
 value: "/tmp/config/jenkins-sa"
 volumeMounts:
 - name: secret-volume
 readOnly: true
 mountPath: "/tmp/config" 
 args:
 - infinity
```

Since our pod needs access to a remote Kubernetes cluster, I have mounted a service account KUBECONFIG into the pod as a secret.

1. Fetch from GitHub
 ``` stage('Fetch from GitHub') {
 steps {
 dir("app"){
 git(
 poll: true,
 changelog: true,
 branch: "main",
 credentialsId: "git-jenkins",
 url: "git@github.com:jeffellin/${APP_NAME}.git"
 )
 sh 'git rev-parse HEAD > git-commit.txt'
 }
 }
 }
 ```

2. Create an Image with TBS. Use the `-w` flag to wait until the build is complete.

 ``` 
 stage('Create Image') {
 steps {
 container('k8s') {
 sh '''#!/bin/sh -e
 export GIT_COMMIT=$(cat app/git-commit.txt)
 kp image save ${APP_NAME} \
 --git git@github.com:jeffellin/${APP_NAME}.git \
 -t harbor.ellin.net/dev/${APP_NAME} \
 --env BP_GRADLE_BUILD_ARGUMENTS='--no-daemon build' \
 --git-revision ${GIT_COMMIT} -w
 '''
 } 
 ````

_Images Monitored by TBS_

![TBS Images](/wp-content/uploads/2021/tbs.png)

3. Update Deployment Manifest

 Since we use `Kustomize` to maintain our deployment versions. We update the `kustomization.yaml` using `kustomize` itself.

 ```
 stage('Update Deployment Manifest'){
 steps {
 container('k8s'){
 dir("gitops"){
 git(
 poll: false,
 changelog: false,
 branch: "master",
 credentialsId: "git-jenkins",
 url: "git@github.com:jeffellin/spring-petclinic-gitops.git"
 )
 }
 sshagent(['git-jenkins']) { 
 sh '''#!/bin/sh -e
 
 kubectl get image ${APP_NAME} -o json | jq -r .status.latestImage >> containerversion.txt
 export CONTAINER_VERSION=$(cat containerversion.txt)
 cd gitops/app
 kustomize edit set image ${APP_NAME}=${CONTAINER_VERSION}
 git config --global user.name "jenkins CI"
 git config --global user.email "none@none.com"
 git add .
 git diff-index --quiet HEAD || git commit -m "update by ci"
 mkdir -p ~/.ssh
 ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts
 git pull -r origin master
 git push --set-upstream origin master
 '''
 } 
 }}}
 ```

All of these steps run within the "k8s" container, which was pulled from Harbor.

harbor.ellin.net/library/docker-build

This image is based on the following Dockerfile. 

```
FROM docker:dind

ENV KUBE_VERSION 1.20.4
ENV HELM_VERSION 3.5.3
ENV KP_VERSION 0.2.0
RUN apk add --no-cache ca-certificates bash git openssh curl jq bind-tools subversion git-svn \
 && wget -q https://storage.googleapis.com/kubernetes-release/release/v${KUBE_VERSION}/bin/linux/amd64/kubectl -O /usr/local/bin/kubectl \
 && chmod +x /usr/local/bin/kubectl \
 && wget -q https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz -O - | tar -xzO linux-amd64/helm > /usr/local/bin/helm \
 && chmod +x /usr/local/bin/helm \
 && chmod g+rwx /root \
 && mkdir /config \
 && chmod g+rwx /config \
 && helm repo add "stable" "https://charts.helm.sh/stable" --force-update

RUN wget https://github.com/vmware-tanzu/kpack-cli/releases/download/v${KP_VERSION}/kp-linux-${KP_VERSION}
RUN mv kp-linux-${KP_VERSION} /usr/local/bin/kp
RUN chmod a+x /usr/local/bin/kp

RUN curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
RUN mv kustomize /usr/local/bin

#ADD ca.crt /usr/local/share/ca-certificates
#RUN update-ca-certificates
WORKDIR /config

CMD bash
```

It's a standard docker image with some utilities that we commonly need to use. 

 * bash
 * git
 * openssh
 * curl
 * jq
 * helm
 * kubectl
 * kustomize

The complete script is available [here](https://raw.githubusercontent.com/jeffellin/spring-petclinic-microservices/k8s/ci/jenkins/pipelines/spring-boot-app.pipeline)

All the source code for the `pet-clinic` and its deployment are available on [GitHub](https://github.com/jeffellin/spring-petclinic-microservices/tree/k8s)

In the following article, I will include an example of how to automate promoting artifacts to a stage/prod environment using GitOps.